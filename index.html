<!DOCTYPE html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js">
</script>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta property="og:description" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta name="twitter:description"
    content="We propose a new imitation learning algorithm that substantially improves sample efficiency for continuous control problems in simulation and on real-world robotic manipulation tasks.">
  <meta name="twitter:image" content="./mfiles/arch bet.001.png" />
  <link rel="shortcut icon" href="img/favicon.png">
  <link rel="stylesheet" href="css/simple-grid.css">
  <title>Watch and Match: Supercharging Imitation with Regularized Optimal Transport</title>
</head>

<body>
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-13 center">
          <h1>Watch and Match: Supercharging Imitation with Regularized Optimal Transport</h1>
        </div>
        <div class="col-3 hidden-sm"></div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://arxiv.org/abs/2206.15469" download>
            <h3 style="color: #F5A803">Paper</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://osf.io/4w69f/?view_only=e29b9dc9ea474d038d533c2245754f0c" download>
            <h3 style="color: #F5A803">Data</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://github.com/siddhanthaldar/ROT" download>
            <h3 style="color: #F5A803">Code</h3>
          </a>
        </div>
      </div>

      <!-- Author names -->
      <div class="row">
        <!-- <div class="col-0 hidden-sm"></div> -->
        <div class="col-3 center">
          <a style="text-decoration: none">
            <h3>Siddhant Haldar</h3>
            <p>New York University</p>
          </a>
        </div>
        <div class="col-3 center">
          <a style="text-decoration: none">
            <h3>Vaibhav Mathur</h3>
            <p>New York University</p>
          </a>
        </div>
        <div class="col-3 center">
          <a style="text-decoration: none">
            <h3>Denis Yarats</h3>
            <p>New York University</p>
          </a>
        </div>
        <div class="col-3 center">
          <a style="text-decoration: none">
            <h3>Lerrel Pinto</h3>
            <p>New York University</p>
          </a>
        </div>
      </div>
      
      <!--Intro video-->
      <div class="intro-vid">
        <div class="container">
          <div class="col-12">
            <!-- <video class="img" style="height: 600" controls loop>
              <source src="./mfiles/videos/ROT_full.mp4" type="video/mp4">
            </video> -->
            <body>
              <iframe width="711" height="400" src="https://www.youtube.com/embed/b0xaoHj3IJs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </body>
          </div>
        </div>
      </div>

      <!--Abstract-->
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Abstract</h2>
          <p>
            Imitation learning holds tremendous promise in learning policies efficiently for
            complex decision making problems. Current state-of-the-art algorithms often use
            inverse reinforcement learning (IRL), where given a set of expert demonstrations,
            an agent alternatively infers a reward function and the associated optimal policy.
            However, such IRL approaches often require substantial online interactions for
            complex control problems. In this work, we present Regularized Optimal Transport
            (ROT), a new imitation learning algorithm that builds on recent advances in optimal
            transport based trajectory-matching. Our key technical insight is that adaptively
            combining trajectory-matching rewards with behavior cloning can significantly
            accelerate imitation even with only a few demonstrations. Our experiments on 20
            visual control tasks across the DeepMind Control Suite, the OpenAI Robotics Suite,
            and the Meta-World Benchmark demonstrate an average of 7.8× faster imitation
            to reach 90% of expert performance compared to prior state-of-the-art methods.
            On real-world robotic manipulation, with just one demonstration and an hour of
            online training, ROT achieves an average success rate of 90.1% across 14 tasks.
          </p>
        </div>
      </div>
    </div>
    
    <!--Image-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Method</h2>
          <p><a style="color: #00A2FF; font-weight: bold;">Regularized Optimal Transport (ROT)</a> is a new imitation learning 
            algorithm that adaptively combines offline behavior cloning with online trajectory-matching based rewards (top). 
            This enables signficantly faster imitation across a variety of simulated and real robotics tasks, while being 
            compatible with high-dimensional visual observation. On our xArm robot, ROT can learn visual policies with only 
            a single human demonstration and under an hour of online training.
        </div>
      </div>
    </div>
    <div class="row">
      <div class="center img">
        <img src="./mfiles/corl_intro_fig.png" style="max-width:1500px;width:50%" frameborder="0"
          allowfullscreen></img>
      </div>
    </div>

    <!--Method-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <p>Our main findings can be summarized as:
          <ul style="font-size: 1.125rem;font-weight: 200;line-height: 1.8">
            <li>ROT outperforms prior state-of-the-art imitation methods, reaching 90% of expert performance
              7.8× faster than our strongest baselines on simulated visual control benchmarks.</li>
            <li>On real-world tasks, with a single human demonstration and an hour of training, ROT achieves
              an average success rate of 90.1% with randomized robot initialization and image observations.
              This is significantly higher than behavior cloning (36.1%) and adversarial IRL (14.6%) based
              approaches.</li>
            <li>ROT exceeds the performance of state-of-the-art RL trained with rewards, while coming close to
              methods that augment RL with demonstrations. Unlike standard RL methods, ROT does not require 
              hand-specification of the reward function.</li>
            <li>Ablation studies demonstrate the importance of every component in ROT, particularly the role
              that soft Q-filtering plays in stabilizing training and the need for OT-based rewards during online
              learning.</li>
          </ul>
          </p>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Robot Results</h2>
          <p>We provide evaluation rollouts of ROT on a set of 14 real-world manipulation tasks. With just one demonstration and one hour of
            online training, ROT achieved an average sucess rate of 90.1% across 14 tasks. This is significantly higher than behavior 
            cloning (36.1%) and adversarial IRL (14.6%) based approaches.
          </p>
        </div>
        <img class="center" src="./mfiles/gifs/combined_gif_cropped.gif" style="width: 100%;">
        <br><br>
        <img class="center" src="./mfiles/robot_exp_combined_cropped.png" style="width:100%;"></img>
        <img class="center" src="./mfiles/robot_exp_legend.png" style="width:100%;"></img>
        <!-- <img class="center" src="./mfiles/robot_exp_combined.png" style="width:100%"></img> -->
        <br><br>
        <!-- <img class="center" src="./mfiles/robot_results.png" style="width:100%"></img> -->
        <!-- </div> -->
      </div>
    </div>
    <!--Experiments-->
    <div class="c">
      <input type="radio" name="a" id="cr-1">
      <label for="cr-1" style="--hue: 82"></label>
      <div class="ci" style="--z: 6">
        <img src="./mfiles/gifs/diff_starts/eraseboard.gif" alt="Trees">
      </div>
    
      <input type="radio" name="a" id="cr-2">
      <label for="cr-2" style="--hue: 40"></label>
      <div class="ci" style="--z: 5">
        <img src="./mfiles/gifs/diff_starts/knobrotate.gif" alt="Mountains and houses">
      </div>
    
      <input type="radio" name="a" id="cr-3">
      <label for="cr-3" style="--hue: 210"></label>
      <div class="ci" style="--z: 4">
        <img src="./mfiles/gifs/diff_starts/peginsert.gif" alt="Sky and mountains">
      </div>

      <input type="radio" name="a" id="cr-4">
      <label for="cr-4" style="--hue: 210"></label>
      <div class="ci" style="--z: 3">
        <img src="./mfiles/gifs/diff_starts/hangbag.gif" alt="Sky and mountains">
      </div>

      <input type="radio" name="a" id="cr-5">
      <label for="cr-5" style="--hue: 210"></label>
      <div class="ci" style="--z: 2">
        <img src="./mfiles/gifs/diff_starts/boxopen.gif" alt="Sky and mountains">
      </div>

      <input type="radio" name="a" id="cr-6" checked>
      <label for="cr-6" style="--hue: 32"></label>
      <div class="ci" style="--z: 1">
        <img src="./mfiles/gifs/diff_starts/buttonpress.gif" alt="Snow on leafs">
      </div>

    </div>
    
    <div class="container" style="padding-bottom: 40px; padding-top: 0px">
      <div class="row">
        <div class="center m-bottom">
          <h5> ROT performs well across varied start positions </h5>
        </div>
      </div>
    </div>

    <div class="d">
      <input type="radio" name="b" id="dr-1" checked>
      <label for="dr-1" style="--hue: 32"></label>
      <div class="di" style="--z: 1">
        <img src="./mfiles/gifs/diff_starts/failures.gif" alt="Snow on leafs">
      </div>
    </div>

    <div class="container" style="padding-bottom: 40px; padding-top: 0px">
      <div class="row">
        <div class="center m-bottom">
          <h5> ROT failure cases </h5>
        </div>
      </div>
    </div>

    <!--Experiments-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <!-- <h2 class="center m-bottom">Experiments</h2>
          <p>To demonstrate the effectiveness of ROT, we run extensive experiments on 20 simulated tasks across
            DM Control, OpenAI Robotics, and Meta-world, and 14 robotic manipulation tasks on an xArm. For DM Control, we 
            measure the average episode reward. For OpenAI Robotics, Meta-world and the real-world xArm tasks, we measure the number
            of successful trajectories. Evaluations are over 10 trajectories for the simulated tasks and 20 trajectories for the 
            real-world tasks. To reach 90% of expert performance, ROT is on average
          </p> -->
          <h2 class="center m-bottom">Simulation Results</h2>
          <p>Our experiments on 20 tasks across the DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World Benchmark,
              demonstrate an average of 7.8× faster imitation to reach 90% of expert performance compared to prior state-of-the-art
              methods. Individually, to reach 90% of expert performance, ROT is on average
          </p>
          <ul style="font-size: 1.125rem;font-weight: 200;line-height: 1.8">
            <li> 8.7× faster on DeepMind Control tasks</li>
            <img class="center" src="./mfiles/appendix_pixel_results_dmc.png" style="width:100%"></img>
            <img class="center" src="./mfiles/sim_result_legend.png" style="width:100%;"></img>
            <li>2.1× faster on OpenAI Robotics tasks</li>
            <img class="center" src="./mfiles/appendix_pixel_results_fetch.png" style="width:100%"></img>
            <img class="center" src="./mfiles/sim_result_legend.png" style="width:100%;"></img>
            <li>8.9× faster on Meta-world tasks</li>
            <img class="center" src="./mfiles/appendix_pixel_results_metaworld.png" style="width:100%"></img>
            <img class="center" src="./mfiles/sim_result_legend.png" style="width:100%;"></img>
          </ul>
          </p>
        </div>
      </div>
    </div>

    <!--Future Work-->
    <!-- <div class="container" style="padding-bottom: 150px; padding-top: 20px">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Limitations and Future Work</h2>
          <p>In this work, we have proposed a new imitation learning algorithm, ROT, that demonstrates improved
            performance compared to prior state-of-the-art work on a variety of simulated and robotic domains.
            However, we recognize a few limitations in this work: (a) Since our OT-based approach aligns agents
            with demonstrations without task-specific rewards, it relies on the demonstrator being an ‘expert’.
            Extending ROT to suboptimal, noisy and multimodal demonstrations would be an exciting problem to
            tackle. (b) Performing BC pretraining and BC-based regularization requires access to expert actions,
            which may not be present in some real-world scenarios particularly when learning from humans.
            Recent work on using inverse models to infer actions given observational data could alleviate this
            challenge. (c) On robotic tasks such as Peg in box (hard) and Pressing a switch from Fig. 3, we
            find that ROT’s performance drops substantially compared to other tasks. This might be due to the
            lack of visual features corresponding to the task success. For example, in the ‘Peg’ task, it is visually
            difficult to discriminate if the peg is in the box or behind the box. Similarly for the ‘Switch’ task, it is
            difficult to discern if the button was pressed or not. This limitation can be addressed by integrating
            more sensory modalities such as additional cameras, and tactile sensors in the observation space.
          </p>
        </div>
      </div>
    </div> -->

    <div class="row">
      <div class="col-1 hidden-sm"></div>
      <div class="col-10">
        <h2 class="center">Citation</h3>
        <pre class="code">
          @article{haldar2022watch,
                    title={Watch and Match: Supercharging Imitation with Regularized Optimal Transport},
                    author={Haldar, Siddhant and Mathur, Vaibhav and Yarats, Denis and Pinto, Lerrel},
                    journal={CoRL},
                    year={2022}
                  }         
        </pre>
      </div>
      <div class="col-1 hidden-sm"></div>
    </div>

  </div>
  <footer>
  </footer>
</body>

</html>
